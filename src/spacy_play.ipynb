{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vectors and Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acknowledgements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://spacy.io/\n",
    "- https://spacy.io/usage/spacy-101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy as pkg_spacy\n",
    "import scipy.spatial as pkg_spatial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Work: Small Size Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English tokenizer, tagger, parser and NER\n",
    "nlp_sm = pkg_spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun phrases: ['Sebastian Thrun', 'self-driving cars', 'Google', 'few people', 'the company', 'him', 'I', 'you', 'very senior CEOs', 'major American car companies', 'my hand', 'I', 'Thrun', 'an interview', 'Recode']\n",
      "Verbs: ['start', 'work', 'drive', 'take', 'tell', 'shake', 'turn', 'talk', 'say']\n",
      "Sebastian Thrun PERSON\n",
      "2007 DATE\n",
      "American NORP\n",
      "Thrun GPE\n",
      "Recode ORG\n",
      "earlier this week DATE\n"
     ]
    }
   ],
   "source": [
    "# Process whole documents\n",
    "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
    "        \"Google in 2007, few people outside of the company took him \"\n",
    "        \"seriously. “I can tell you very senior CEOs of major American \"\n",
    "        \"car companies would shake my hand and turn away because I wasn’t \"\n",
    "        \"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
    "        \"this week.\")\n",
    "doc = nlp_sm(text)\n",
    "\n",
    "# Analyze syntax\n",
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "\n",
    "# Find named entities, phrases and concepts\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple PROPN nsubj\n",
      "is AUX aux\n",
      "looking VERB ROOT\n",
      "at ADP prep\n",
      "buying VERB pcomp\n",
      "U.K. PROPN dobj\n",
      "startup NOUN dobj\n",
      "for ADP prep\n",
      "$ SYM quantmod\n",
      "1 NUM compound\n",
      "billion NUM pobj\n",
      "Apple Apple PROPN NNP nsubj Xxxxx True False\n",
      "is be AUX VBZ aux xx True True\n",
      "looking look VERB VBG ROOT xxxx True False\n",
      "at at ADP IN prep xx True True\n",
      "buying buy VERB VBG pcomp xxxx True False\n",
      "U.K. U.K. PROPN NNP dobj X.X. False False\n",
      "startup startup NOUN NN dobj xxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "$ $ SYM $ quantmod $ False False\n",
      "1 1 NUM CD compound d False False\n",
      "billion billion NUM CD pobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "doc = nlp_sm(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)\n",
    "\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Work: Medium Size Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Medium size package\n",
    "nlp_md = pkg_spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Work: Large Size Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Large Package\n",
    "nlp_lg = pkg_spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog True 75.254234 False\n",
      "cat True 63.188496 False\n",
      "banana True 31.620354 False\n",
      "afskfsd False 0.0 True\n"
     ]
    }
   ],
   "source": [
    "tokens = nlp_lg(\"dog cat banana afskfsd\")\n",
    "\n",
    "for token in tokens:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)\n",
    "\n",
    "# for token in tokens:\n",
    "#     print(token.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like salty fries and hamburgers. <-> Fast food tastes very good. 0.6871286202797843\n",
      "salty fries <-> hamburgers 0.6901010870933533\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp_lg(\"I like salty fries and hamburgers.\")\n",
    "doc2 = nlp_lg(\"Fast food tastes very good.\")\n",
    "\n",
    "# Similarity of two documents\n",
    "print(doc1, \"<->\", doc2, doc1.similarity(doc2))\n",
    "\n",
    "# Similarity of tokens and spans\n",
    "french_fries = doc1[2:4]\n",
    "burgers = doc1[5]\n",
    "\n",
    "print(french_fries, \"<->\", burgers, french_fries.similarity(burgers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king_vector_norm = 69.68691461050098, man_vector_norm = 68.7250111232666, diff_vector_norm = 0.9619034872343803, similarity = 0.4166158683466932\n"
     ]
    }
   ],
   "source": [
    "king_doc = nlp_lg(\"king\")\n",
    "man_doc = nlp_lg(\"man\")\n",
    "\n",
    "print (\"king_vector_norm = {}, man_vector_norm = {}, diff_vector_norm = {}, similarity = {}\".format( \\\n",
    "    king_doc.vector_norm, man_doc.vector_norm, abs(king_doc.vector_norm - man_doc.vector_norm), king_doc.similarity(man_doc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**:\n",
    "Vector Norm (vector_norm) values do not reflect the similary. For example, \"king\" and \"man\" have _similar_ vector_norm values (esp. diff < 1), but similarity is _not_ high. So, for doing vector arithmetic, use the full vector than vector norm value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity = lambda x, y: 1 - pkg_spatial.distance.cosine(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity (king, king) = (69.68691461050098, 69.68691461050098):\n",
      "\t(diff=0.0, similarity=1.0, cosine_similarity=1)\n",
      "Similarity (king, queen) = (69.68691461050098, 44.440847813672285):\n",
      "\t(diff=25.246066796828693, similarity=0.6108841234425123, cosine_similarity=0.6108841300010681)\n",
      "Similarity (king, ruler) = (69.68691461050098, 52.792574722108114):\n",
      "\t(diff=16.894339888392864, similarity=0.7641486322987932, cosine_similarity=0.7641486525535583)\n",
      "Similarity (king, man) = (69.68691461050098, 68.7250111232666):\n",
      "\t(diff=0.9619034872343803, similarity=0.4166158683466932, cosine_similarity=0.41661590337753296)\n",
      "Similarity (king, woman) = (69.68691461050098, 53.99287730357096):\n",
      "\t(diff=15.69403730693002, similarity=0.3572872976296661, cosine_similarity=0.3572872281074524)\n",
      "Similarity (king, human) = (69.68691461050098, 63.08981987699371):\n",
      "\t(diff=6.597094733507269, similarity=0.2968272202890541, cosine_similarity=0.2968272268772125)\n",
      "Similarity (queen, queen) = (44.440847813672285, 44.440847813672285):\n",
      "\t(diff=0.0, similarity=1.0, cosine_similarity=1)\n",
      "Similarity (queen, ruler) = (44.440847813672285, 52.792574722108114):\n",
      "\t(diff=8.351726908435829, similarity=0.4851217666110863, cosine_similarity=0.48512178659439087)\n",
      "Similarity (queen, man) = (44.440847813672285, 68.7250111232666):\n",
      "\t(diff=24.284163309594312, similarity=0.35414849045437796, cosine_similarity=0.3541484773159027)\n",
      "Similarity (queen, woman) = (44.440847813672285, 53.99287730357096):\n",
      "\t(diff=9.552029489898672, similarity=0.47567791204078347, cosine_similarity=0.47567787766456604)\n",
      "Similarity (queen, human) = (44.440847813672285, 63.08981987699371):\n",
      "\t(diff=18.648972063321423, similarity=0.06645220084727563, cosine_similarity=0.06645218282938004)\n",
      "Similarity (ruler, ruler) = (52.792574722108114, 52.792574722108114):\n",
      "\t(diff=0.0, similarity=1.0, cosine_similarity=1)\n",
      "Similarity (ruler, man) = (52.792574722108114, 68.7250111232666):\n",
      "\t(diff=15.932436401158483, similarity=0.3277244301287238, cosine_similarity=0.3277244567871094)\n",
      "Similarity (ruler, woman) = (52.792574722108114, 53.99287730357096):\n",
      "\t(diff=1.200302581462843, similarity=0.28001609023557705, cosine_similarity=0.28001609444618225)\n",
      "Similarity (ruler, human) = (52.792574722108114, 63.08981987699371):\n",
      "\t(diff=10.297245154885594, similarity=0.20345380531031643, cosine_similarity=0.20345379412174225)\n",
      "Similarity (man, man) = (68.7250111232666, 68.7250111232666):\n",
      "\t(diff=0.0, similarity=1.0, cosine_similarity=1)\n",
      "Similarity (man, woman) = (68.7250111232666, 53.99287730357096):\n",
      "\t(diff=14.73213381969564, similarity=0.8273443215651575, cosine_similarity=0.8273441791534424)\n",
      "Similarity (man, human) = (68.7250111232666, 63.08981987699371):\n",
      "\t(diff=5.635191246272889, similarity=0.2533094463340689, cosine_similarity=0.253309428691864)\n",
      "Similarity (woman, woman) = (53.99287730357096, 53.99287730357096):\n",
      "\t(diff=0.0, similarity=1.0, cosine_similarity=1)\n",
      "Similarity (woman, human) = (53.99287730357096, 63.08981987699371):\n",
      "\t(diff=9.096942573422751, similarity=0.269444287990343, cosine_similarity=0.2694442570209503)\n",
      "Similarity (human, human) = (63.08981987699371, 63.08981987699371):\n",
      "\t(diff=0.0, similarity=1.0, cosine_similarity=1)\n"
     ]
    }
   ],
   "source": [
    "words = [\"king\", \"queen\", \"ruler\", \"man\", \"woman\", \"human\"]\n",
    "docs = []\n",
    "\n",
    "for word in words:\n",
    "    doc = nlp_lg(word)\n",
    "    docs.append(doc)\n",
    "\n",
    "#print(words)\n",
    "#print(vectors)\n",
    "\n",
    "# Similarities\n",
    "for i in range(len(docs)):\n",
    "    for j in range(len(docs)-i):\n",
    "        print (\"Similarity ({}, {}) = ({}, {}):\\n\\t(diff={}, similarity={}, cosine_similarity={})\".format(\\\n",
    "            words[i], words[i+j], docs[i].vector_norm, docs[i+j].vector_norm, \\\n",
    "            abs(docs[i].vector_norm - docs[i+j].vector_norm), docs[i].similarity(docs[i+j]), \\\n",
    "            cosine_similarity(docs[i].vector, docs[i+j].vector)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words near to K2Q\n",
      "Similarity (queen, king-man+woman) = 0.6178014278411865\n",
      "Word = ta, Similiarity = -0.2223987877368927, Diff Simialrity = 0.8402002155780792\n",
      "Word = cos, Similiarity = -0.21016256511211395, Diff Simialrity = 0.8279639929533005\n",
      "Word = ai, Similiarity = -0.20889312028884888, Diff Simialrity = 0.8266945481300354\n",
      "Word = cuz, Similiarity = -0.20390625298023224, Diff Simialrity = 0.8217076808214188\n",
      "Word = em, Similiarity = -0.14783941209316254, Diff Simialrity = 0.7656408399343491\n",
      "Word = nuff, Similiarity = -0.14137345552444458, Diff Simialrity = 0.7591748833656311\n",
      "Word = coz, Similiarity = -0.1318208873271942, Diff Simialrity = 0.7496223151683807\n",
      "Word = got, Similiarity = -0.12722618877887726, Diff Simialrity = 0.7450276166200638\n",
      "Word = doin, Similiarity = -0.1028217151761055, Diff Simialrity = 0.720623143017292\n",
      "Word = ca, Similiarity = -0.10074420273303986, Diff Simialrity = 0.7185456305742264\n",
      "Words near to Q2K\n",
      "Similarity (king, queen-woman+man) = 0.6300731301307678\n",
      "Word = v, Similiarity = -0.10036450624465942, Diff Simialrity = 0.7304376363754272\n",
      "Word = co, Similiarity = -0.08563277870416641, Diff Simialrity = 0.7157059088349342\n",
      "Word = vs, Similiarity = -0.08130335062742233, Diff Simialrity = 0.7113764807581902\n",
      "Word = m, Similiarity = -0.07530508935451508, Diff Simialrity = 0.7053782194852829\n",
      "Word = pm, Similiarity = -0.07525051385164261, Diff Simialrity = 0.7053236439824104\n",
      "Word = x, Similiarity = -0.06688850373029709, Diff Simialrity = 0.6969616338610649\n",
      "Word = e, Similiarity = -0.06599827110767365, Diff Simialrity = 0.6960714012384415\n",
      "Word = are, Similiarity = -0.04820999875664711, Diff Simialrity = 0.6782831288874149\n",
      "Word = c, Similiarity = -0.04660439491271973, Diff Simialrity = 0.6766775250434875\n",
      "Word = g, Similiarity = -0.04612366482615471, Diff Simialrity = 0.6761967949569225\n"
     ]
    }
   ],
   "source": [
    "king_vec = nlp_lg.vocab.get_vector(\"king\")\n",
    "man_vec = nlp_lg.vocab.get_vector(\"man\")\n",
    "woman_vec = nlp_lg.vocab.get_vector(\"woman\")\n",
    "queen_vec = nlp_lg.vocab.get_vector(\"queen\")\n",
    "\n",
    "k2q_vec = king_vec - man_vec + woman_vec\n",
    "queen_similiarity = cosine_similarity(queen_vec, k2q_vec)\n",
    "\n",
    "q2k_vec = queen_vec - woman_vec + man_vec\n",
    "king_similiarity = cosine_similarity(king_vec, q2k_vec)\n",
    "\n",
    "# Find the first ten closest vectors in the vocabulary to the computed vectors\n",
    "k2q_similarities = []\n",
    "q2k_similarities = []\n",
    "\n",
    "for word in nlp_lg.vocab:\n",
    "    if word.has_vector:\n",
    "        if word.is_lower:\n",
    "            if word.is_alpha:\n",
    "                k2q_similarity = cosine_similarity(k2q_vec, word.vector)\n",
    "                k2q_similarities.append({ \"word\" : word, \"similarity\" : k2q_similarity})\n",
    "\n",
    "                q2k_similarity = cosine_similarity(q2k_vec, word.vector)\n",
    "                q2k_similarities.append({ \"word\" : word, \"similarity\" : q2k_similarity})\n",
    "\n",
    "k2q_similarities = sorted(k2q_similarities, key=lambda entry: entry[\"similarity\"])\n",
    "q2k_similarities = sorted(q2k_similarities, key=lambda entry: entry[\"similarity\"])\n",
    "\n",
    "print (\"Words near to K2Q\")\n",
    "print (\"Similarity (queen, king-man+woman) = {}\".format(queen_similiarity))\n",
    "for i in range(10):\n",
    "    entry = k2q_similarities[i]\n",
    "    print(\"Word = {}, Similiarity = {}, Diff Simialrity = {}\".format(\\\n",
    "        entry[\"word\"].text, entry[\"similarity\"], abs(queen_similiarity - entry[\"similarity\"])))\n",
    "\n",
    "print (\"Words near to Q2K\")\n",
    "print (\"Similarity (king, queen-woman+man) = {}\".format(king_similiarity))\n",
    "for i in range(10):\n",
    "    entry = q2k_similarities[i]\n",
    "    print(\"Word = {}, Similiarity = {}, Diff Simialrity = {}\".format(\\\n",
    "        entry[\"word\"].text, entry[\"similarity\"], abs(king_similiarity - entry[\"similarity\"])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Puzzles**:\n",
    "- Why is cosine value > 1? If that's not the case, how come (1-cosine) is negative?\n",
    "- The model version that has been downloaded when this exercise ran seems to be very badly trained based on above outputs\n",
    "- For example, for q2k vector, nearest words are \"v\", \"m\", \"x\", \"e\", \"c\", \"g\" (which are single letter and not meaningful)\n",
    "- Similarly, for k2q vector, nearest words are \"ta\", \"ai\", \"ca\" which are not meaningful words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
